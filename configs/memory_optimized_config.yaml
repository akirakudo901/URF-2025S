model_config:
  vocab_size: 50257  # GPT2 vocabulary size
  d_model: 768       # GPT2 model dimension
  num_embeddings: 512  # VQ codebook size
  commitment_cost: 0.25  # VQ commitment cost
  aggregation_hidden_dim: 1024  # Aggregation MLP hidden dim
  num_thoughts: 32   # Number of parallel sequences
  n_positions: 1024   # Maximum sequence length

training_config:
  learning_rate: 1e-4
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  use_lr_scheduler: true
  min_lr: 1e-6
  num_epochs: 50
  batch_size: 1  # Very small batch size for memory efficiency
  gradient_clip: 1.0
  vq_loss_weight: 1.0
  quantize_cot_only: true
  save_every: 5
  checkpoint_dir: checkpoints/gpt2vqvae_memory_optimized
  pad_token_id: 50256
  val_split: 0.1
  
  # Memory optimization settings
  use_mixed_precision: true
  gradient_accumulation_steps: 8  # Effective batch size = 1 * 8 = 8
  use_gradient_checkpointing: true
  max_memory_gb: 2.0  # Conservative memory limit for dataset caching
  use_dynamic_batching: false
  max_tokens_per_batch: 4096  # Reduced for memory efficiency
  max_samples: null  # Set to a number (e.g., 1000) for debugging

data_config:
  data_dir: data/GSM8K 